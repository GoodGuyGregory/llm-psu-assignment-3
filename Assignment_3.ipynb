{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be002d99-2463-46c2-9cc8-2236e78c1e0d",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c6a29-0554-41c6-ba2e-88714b5a40e0",
   "metadata": {},
   "source": [
    "## Load The Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdfa71-edf1-462c-8dd7-08196076f4f9",
   "metadata": {},
   "source": [
    "**Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fcd557-789a-4eb6-918e-5733571739b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.11.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8349e6-ad8f-40ca-a710-564335aead49",
   "metadata": {},
   "source": [
    "### Load French Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa51284-87ac-4731-a200-64e9b3afa0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregwitt/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-3/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    RT @user: Le piège de l’#énergie verte (via @user) http\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# French Validation \n",
    "ds_validation_french = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"french\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_french } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_french['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_french['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9719fa5f",
   "metadata": {},
   "source": [
    "### Split French Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d664a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative french tweets: 108\n",
      "number of positive french tweets: 108\n",
      "\n",
      "Random negative french tweet: Manifestation contre les éoliennes à Foix http\n",
      "random positive french tweet: Mon TRÈS CHER LOUP 400 loups qui coûtent 12 millions d'€ d'indemnité = 30000€ par loup ! Merci les écologistes\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_french_tweets = ds_validation_french.filter(lambda x: x['label'] == 0)\n",
    "positive_french_tweets = ds_validation_french.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative french tweets: {len(negative_french_tweets)}\")\n",
    "print(f\"number of positive french tweets: {len(positive_french_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative french tweet: {negative_french_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive french tweet: {positive_french_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be09296e-705f-4664-b1ce-52e2c1a766ba",
   "metadata": {},
   "source": [
    "### Load German Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a4078a-5a49-4861-8c6a-33da59e3ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    Beim niesen werden alle werden alle Körperfunkionen ausgesetzt sogar das Herz\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# German Validation Tweets\n",
    "ds_validation_german = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"german\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_german } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,324)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_german['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_german['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d21a17",
   "metadata": {},
   "source": [
    "### Split German Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2092cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative german tweets: 108\n",
      "number of positive german tweets: 108\n",
      "\n",
      "Random negative german tweet: Meine ganzen Runtastic Apps wurden nicht wiederhergestellt?!\n",
      "random positive german tweet: RT @user: TTTTTTOOOOOOORRRRR durch Ronny! Nach einer klasse Einzelleistung trifft Herthas Nummer 12 zum 6:1 gegen Eintracht Frankfurt.…\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_german_tweets = ds_validation_german.filter(lambda x: x['label'] == 0)\n",
    "positive_german_tweets = ds_validation_german.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative german tweets: {len(negative_german_tweets)}\")\n",
    "print(f\"number of positive german tweets: {len(positive_german_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative german tweet: {negative_german_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive german tweet: {positive_german_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f504a2b-a047-476e-acf5-1a59e6c3fb0d",
   "metadata": {},
   "source": [
    "### Load Hindi Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eacde19e-108e-4dda-a3f8-82f94b589bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    match hrane me guddu ka maa ka bahut bada hath hota agr guddu ke papa rokte nhi  .  .  . \n",
      "\n",
      "    Label: \n",
      "    2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Hindi Validation Tweets\n",
    "ds_validation_hindi = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"hindi\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_hindi } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_hindi['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_hindi['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35125781",
   "metadata": {},
   "source": [
    "### Split Hindi Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d2936a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative hindi tweets: 108\n",
      "number of positive hindi tweets: 108\n",
      "\n",
      "Random negative hindi tweet: awesome hillarious or no words can describe this great comedy\n",
      "random positive hindi tweet: victor cruz isnt himself today .  .  .  .  .  .  .  . i know he'll get it together in the 2nd half\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_hindi_tweets = ds_validation_hindi.filter(lambda x: x['label'] == 0)\n",
    "positive_hindi_tweets = ds_validation_hindi.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative hindi tweets: {len(negative_hindi_tweets)}\")\n",
    "print(f\"number of positive hindi tweets: {len(positive_hindi_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative hindi tweet: {negative_hindi_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive hindi tweet: {positive_hindi_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005eba5-5c2b-437f-932a-8dca157ba6f2",
   "metadata": {},
   "source": [
    "### Load Italian Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "359c8a26-4340-4c35-891e-ab80a6fd1ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    Province: dal vertice fiorentino dei \"dieci\" parte la proposta al governo Monti http #o3news\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# German Validation Tweets\n",
    "ds_validation_italian = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"italian\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_italian } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_italian['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_italian['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe8b7a",
   "metadata": {},
   "source": [
    "### Split Italian Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f538fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative italian tweets: 108\n",
      "number of positive italian tweets: 108\n",
      "\n",
      "Random negative italian tweet: Che poi, a guardare bene, l'unica differenza è che #Grillo è delNord, e #Cettolaqualunque del sud. Suv compresi.\n",
      "random positive italian tweet: @user anche a me piace la tua! :))\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_italian_tweets = ds_validation_italian.filter(lambda x: x['label'] == 0)\n",
    "positive_italian_tweets = ds_validation_italian.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative italian tweets: {len(negative_italian_tweets)}\")\n",
    "print(f\"number of positive italian tweets: {len(positive_italian_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative italian tweet: {negative_italian_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive italian tweet: {positive_italian_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06368255-4405-4cf6-a197-4cec8e748c0d",
   "metadata": {},
   "source": [
    "### Load Portuguese Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50e5e408-db8b-4974-bcaa-a11f5a1daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    Carol 😅😅😅😅😅 homens lavem a louca #Encontro\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Portuguese Validation Tweets\n",
    "ds_validation_portuguese = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"portuguese\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_german } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_portuguese['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_portuguese['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfc6f7",
   "metadata": {},
   "source": [
    "### Split Portuguese Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "681dffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative portuguese tweets: 108\n",
      "number of positive portuguese tweets: 108\n",
      "\n",
      "Random negative portuguese tweet: o programa acabou e ninguém tomou um tacacá #MasterChefBR\n",
      "random positive portuguese tweet: O samba é resistência sem fim. Que aula! #ConversacomBial\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_portuguese_tweets = ds_validation_portuguese.filter(lambda x: x['label'] == 0)\n",
    "positive_portuguese_tweets = ds_validation_portuguese.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative portuguese tweets: {len(negative_portuguese_tweets)}\")\n",
    "print(f\"number of positive portuguese tweets: {len(positive_portuguese_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative portuguese tweet: {negative_portuguese_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive portuguese tweet: {positive_portuguese_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a111e8-1d28-46d1-81c3-0b41a06a2273",
   "metadata": {},
   "source": [
    "### Load Spanish Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed089dbb-243e-4d82-b675-77dd2e61bb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    @user Bueno, yo soy de la Real, neutral total\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Portuguese Validation Tweets\n",
    "ds_validation_spanish = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"spanish\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_spanish } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_spanish['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_spanish['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151163f7-4392-4953-9e3e-27debd553776",
   "metadata": {},
   "source": [
    "### Split Spanish Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d1b4c94-32a6-42b1-82f3-4c463849edbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative spanish tweets: 108\n",
      "number of positive spanish tweets: 108\n",
      "\n",
      "random negative spanish tweet: Me habría gustado hacer alguna de las de los Targaryen pero como están  en paradero desconocido pueh no hay imagenes buenas de ellas y\n",
      "random positive spanish tweet: No es triste, es normal\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_spanish_tweets = ds_validation_spanish.filter(lambda x: x['label'] == 0)\n",
    "positive_spanish_tweets = ds_validation_spanish.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative spanish tweets: {len(negative_spanish_tweets)}\")\n",
    "print(f\"number of positive spanish tweets: {len(positive_spanish_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"random negative spanish tweet: {negative_spanish_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive spanish tweet: {positive_spanish_tweets['text'][random_num]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccddf5d",
   "metadata": {},
   "source": [
    "## Multilingual Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383427ef",
   "metadata": {},
   "source": [
    "### OpenAI GPT 4o - Mini \n",
    "\n",
    "[4o Mini Documentation](https://platform.openai.com/docs/models#gpt-4o-mini)   \n",
    "[GPT 4o Model Card](https://openai.com/index/gpt-4o-system-card/)  \n",
    "[Scoring Evaluation Metrics](https://deepgram.com/learn/arc-llm-benchmark-guide)  \n",
    "[LangChain OpenAI Chat Models](https://python.langchain.com/docs/integrations/chat/openai/)\n",
    "\n",
    "**Multilingual Capabilities**\n",
    "\n",
    "According to the documentation the GPT-4 Models are high performance models and as of 2023, with roughly ~8B parameters. It the state of the art flag ship model for their market for small projects. The improved accuracy in other smaller non-english languages has been a company claim based on the documentation the model's metrics are represented below with ARC-Easy scores, and TruthfulQA scores\n",
    "\n",
    "**GPT ARC Easy Language Scores**\n",
    "\n",
    "![GPT ARC Easy Language Scores](./img/GPT-ARC-Easy-Score.png)\n",
    "\n",
    "**GPT TruthfulQA Scores**\n",
    "\n",
    "![GPT TruthfulQA Scores](./img/GPT-TruthfulQA-Score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e12834",
   "metadata": {},
   "source": [
    "### MistralAI NeMo Model\n",
    "\n",
    "[NeMo Model Documentation](https://mistral.ai/news/mistral-nemo/)  \n",
    "[Other Mistral Models](https://docs.mistral.ai/getting-started/models/models_overview/)  \n",
    "[LangChain Mistral Chat Model ](https://python.langchain.com/docs/integrations/chat/mistralai/#invocation)\n",
    "\n",
    "\n",
    "Mistral Nemo is the best small model, state of the art model with 12B. The Model is *\"designed for multilingual applications\"* it's training is explicitly mentioned in the above article for *\"English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi*. The Mistral documentation even boasts about a new tokenizer used for this model specifically **Tekken** which outperformed it's previous models by ~30%. Compared to Llama 3's tokenizer, NeMo's *\"proved more proficient in compressing text for approcimately 85% of all languages.\"*\n",
    "\n",
    "**Tekken Compression**\n",
    "\n",
    "![Tekken Compression](./img/Tekken-Compression-Rate.png)\n",
    "\n",
    "**NeMo Language Metrics**\n",
    "\n",
    "![Language Performance Metrics](./img/Mistral-Nemo-Language-Scores.png)\n",
    "\n",
    "**Open Source Model Performance Metrics**\n",
    "\n",
    "![Mistral Nemo 12B](./img/MIstral-Nemo-12B-Metrics.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1904a1b",
   "metadata": {},
   "source": [
    "**Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96bd8574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61fc6a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-openai langchain-mistralai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8b7fcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# loads the .env file with LLM API Keys\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dad380b8-93ca-4cd2-b0c0-4799aab72047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets API access keys\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd3e00",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
