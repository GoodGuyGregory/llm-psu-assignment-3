{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be002d99-2463-46c2-9cc8-2236e78c1e0d",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c6a29-0554-41c6-ba2e-88714b5a40e0",
   "metadata": {},
   "source": [
    "## Load The Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdfa71-edf1-462c-8dd7-08196076f4f9",
   "metadata": {},
   "source": [
    "**Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fcd557-789a-4eb6-918e-5733571739b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.11.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8349e6-ad8f-40ca-a710-564335aead49",
   "metadata": {},
   "source": [
    "### Load French Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa51284-87ac-4731-a200-64e9b3afa0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregwitt/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-3/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    RT @user: Le piÃ¨ge de lâ€™#Ã©nergie verte (via @user) http\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# French Validation \n",
    "ds_validation_french = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"french\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_french } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_french['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_french['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9719fa5f",
   "metadata": {},
   "source": [
    "### Split French Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d664a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative french tweets: 108\n",
      "number of positive french tweets: 108\n",
      "\n",
      "Random negative french tweet: Manifestation contre les Ã©oliennes Ã  Foix http\n",
      "random positive french tweet: Mon TRÃˆS CHER LOUP 400 loups qui coÃ»tent 12 millions d'â‚¬ d'indemnitÃ© = 30000â‚¬ par loup ! Merci les Ã©cologistes\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_french_tweets = ds_validation_french.filter(lambda x: x['label'] == 0)\n",
    "positive_french_tweets = ds_validation_french.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative french tweets: {len(negative_french_tweets)}\")\n",
    "print(f\"number of positive french tweets: {len(positive_french_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative french tweet: {negative_french_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive french tweet: {positive_french_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be09296e-705f-4664-b1ce-52e2c1a766ba",
   "metadata": {},
   "source": [
    "### Load German Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a4078a-5a49-4861-8c6a-33da59e3ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    Beim niesen werden alle werden alle KÃ¶rperfunkionen ausgesetzt sogar das Herz\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# German Validation Tweets\n",
    "ds_validation_german = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"german\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_german } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,324)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_german['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_german['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d21a17",
   "metadata": {},
   "source": [
    "### Split German Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2092cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative german tweets: 108\n",
      "number of positive german tweets: 108\n",
      "\n",
      "Random negative german tweet: Meine ganzen Runtastic Apps wurden nicht wiederhergestellt?!\n",
      "random positive german tweet: RT @user: TTTTTTOOOOOOORRRRR durch Ronny! Nach einer klasse Einzelleistung trifft Herthas Nummer 12 zum 6:1 gegen Eintracht Frankfurt.â€¦\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_german_tweets = ds_validation_german.filter(lambda x: x['label'] == 0)\n",
    "positive_german_tweets = ds_validation_german.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative german tweets: {len(negative_german_tweets)}\")\n",
    "print(f\"number of positive german tweets: {len(positive_german_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative german tweet: {negative_german_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive german tweet: {positive_german_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f504a2b-a047-476e-acf5-1a59e6c3fb0d",
   "metadata": {},
   "source": [
    "### Load Hindi Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eacde19e-108e-4dda-a3f8-82f94b589bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    match hrane me guddu ka maa ka bahut bada hath hota agr guddu ke papa rokte nhi  .  .  . \n",
      "\n",
      "    Label: \n",
      "    2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Hindi Validation Tweets\n",
    "ds_validation_hindi = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"hindi\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_hindi } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_hindi['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_hindi['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35125781",
   "metadata": {},
   "source": [
    "### Split Hindi Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d2936a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative hindi tweets: 108\n",
      "number of positive hindi tweets: 108\n",
      "\n",
      "Random negative hindi tweet: awesome hillarious or no words can describe this great comedy\n",
      "random positive hindi tweet: victor cruz isnt himself today .  .  .  .  .  .  .  . i know he'll get it together in the 2nd half\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_hindi_tweets = ds_validation_hindi.filter(lambda x: x['label'] == 0)\n",
    "positive_hindi_tweets = ds_validation_hindi.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative hindi tweets: {len(negative_hindi_tweets)}\")\n",
    "print(f\"number of positive hindi tweets: {len(positive_hindi_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative hindi tweet: {negative_hindi_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive hindi tweet: {positive_hindi_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005eba5-5c2b-437f-932a-8dca157ba6f2",
   "metadata": {},
   "source": [
    "### Load Italian Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "359c8a26-4340-4c35-891e-ab80a6fd1ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    Province: dal vertice fiorentino dei \"dieci\" parte la proposta al governo Monti http #o3news\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# German Validation Tweets\n",
    "ds_validation_italian = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"italian\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_italian } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_italian['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_italian['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe8b7a",
   "metadata": {},
   "source": [
    "### Split Italian Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f538fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative italian tweets: 108\n",
      "number of positive italian tweets: 108\n",
      "\n",
      "Random negative italian tweet: Che poi, a guardare bene, l'unica differenza Ã¨ che #Grillo Ã¨ delNord, e #Cettolaqualunque del sud. Suv compresi.\n",
      "random positive italian tweet: @user anche a me piace la tua! :))\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_italian_tweets = ds_validation_italian.filter(lambda x: x['label'] == 0)\n",
    "positive_italian_tweets = ds_validation_italian.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative italian tweets: {len(negative_italian_tweets)}\")\n",
    "print(f\"number of positive italian tweets: {len(positive_italian_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative italian tweet: {negative_italian_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive italian tweet: {positive_italian_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06368255-4405-4cf6-a197-4cec8e748c0d",
   "metadata": {},
   "source": [
    "### Load Portuguese Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50e5e408-db8b-4974-bcaa-a11f5a1daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    Carol ðŸ˜…ðŸ˜…ðŸ˜…ðŸ˜…ðŸ˜… homens lavem a louca #Encontro\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Portuguese Validation Tweets\n",
    "ds_validation_portuguese = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"portuguese\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_german } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_portuguese['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_portuguese['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfc6f7",
   "metadata": {},
   "source": [
    "### Split Portuguese Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "681dffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative portuguese tweets: 108\n",
      "number of positive portuguese tweets: 108\n",
      "\n",
      "Random negative portuguese tweet: o programa acabou e ninguÃ©m tomou um tacacÃ¡ #MasterChefBR\n",
      "random positive portuguese tweet: O samba Ã© resistÃªncia sem fim. Que aula! #ConversacomBial\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_portuguese_tweets = ds_validation_portuguese.filter(lambda x: x['label'] == 0)\n",
    "positive_portuguese_tweets = ds_validation_portuguese.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative portuguese tweets: {len(negative_portuguese_tweets)}\")\n",
    "print(f\"number of positive portuguese tweets: {len(positive_portuguese_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative portuguese tweet: {negative_portuguese_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive portuguese tweet: {positive_portuguese_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a111e8-1d28-46d1-81c3-0b41a06a2273",
   "metadata": {},
   "source": [
    "### Load Spanish Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed089dbb-243e-4d82-b675-77dd2e61bb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    @user Bueno, yo soy de la Real, neutral total\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Portuguese Validation Tweets\n",
    "ds_validation_spanish = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"spanish\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_spanish } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_spanish['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_spanish['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151163f7-4392-4953-9e3e-27debd553776",
   "metadata": {},
   "source": [
    "### Split Spanish Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d1b4c94-32a6-42b1-82f3-4c463849edbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative spanish tweets: 108\n",
      "number of positive spanish tweets: 108\n",
      "\n",
      "random negative spanish tweet: Me habrÃ­a gustado hacer alguna de las de los Targaryen pero como estÃ¡n  en paradero desconocido pueh no hay imagenes buenas de ellas y\n",
      "random positive spanish tweet: No es triste, es normal\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_spanish_tweets = ds_validation_spanish.filter(lambda x: x['label'] == 0)\n",
    "positive_spanish_tweets = ds_validation_spanish.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative spanish tweets: {len(negative_spanish_tweets)}\")\n",
    "print(f\"number of positive spanish tweets: {len(positive_spanish_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"random negative spanish tweet: {negative_spanish_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive spanish tweet: {positive_spanish_tweets['text'][random_num]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccddf5d",
   "metadata": {},
   "source": [
    "## Multilingual Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6694864a",
   "metadata": {},
   "source": [
    "### Chosen Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383427ef",
   "metadata": {},
   "source": [
    "### OpenAI GPT 4o - Mini \n",
    "\n",
    "[4o Mini Documentation](https://platform.openai.com/docs/models#gpt-4o-mini)   \n",
    "[GPT 4o Model Card](https://openai.com/index/gpt-4o-system-card/)  \n",
    "[Scoring Evaluation Metrics](https://deepgram.com/learn/arc-llm-benchmark-guide)  \n",
    "[LangChain OpenAI Chat Models](https://python.langchain.com/docs/integrations/chat/openai/)\n",
    "\n",
    "**Multilingual Capabilities**\n",
    "\n",
    "According to the documentation the GPT-4 Models are high performance models and as of 2023, with roughly ~8B parameters. It the state of the art flag ship model for their market for small projects. The improved accuracy in other smaller non-english languages has been a company claim based on the documentation the model's metrics are represented below with ARC-Easy scores, and TruthfulQA scores\n",
    "\n",
    "**GPT ARC Easy Language Scores**\n",
    "\n",
    "![GPT ARC Easy Language Scores](./img/GPT-ARC-Easy-Score.png)\n",
    "\n",
    "**GPT TruthfulQA Scores**\n",
    "\n",
    "![GPT TruthfulQA Scores](./img/GPT-TruthfulQA-Score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e12834",
   "metadata": {},
   "source": [
    "### MistralAI NeMo Model\n",
    "\n",
    "[NeMo Model Documentation](https://mistral.ai/news/mistral-nemo/)  \n",
    "[Other Mistral Models](https://docs.mistral.ai/getting-started/models/models_overview/)  \n",
    "[LangChain Mistral Chat Model ](https://python.langchain.com/docs/integrations/chat/mistralai/#invocation)\n",
    "\n",
    "\n",
    "\n",
    "Mistral Nemo is the best small model, state of the art model with 12B. The Model is *\"designed for multilingual applications\"* it's training is explicitly mentioned in the above article for *\"English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi*. The Mistral documentation even boasts about a new tokenizer used for this model specifically **Tekken** which outperformed it's previous models by ~30%. Compared to Llama 3's tokenizer, NeMo's *\"proved more proficient in compressing text for approcimately 85% of all languages.\"*\n",
    "\n",
    "**Tekken Compression**\n",
    "\n",
    "![Tekken Compression](./img/Tekken-Compression-Rate.png)\n",
    "\n",
    "**NeMo Language Metrics**\n",
    "\n",
    "![Language Performance Metrics](./img/Mistral-Nemo-Language-Scores.png)\n",
    "\n",
    "**Open Source Model Performance Metrics**\n",
    "\n",
    "![Mistral Nemo 12B](./img/MIstral-Nemo-12B-Metrics.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a65fe8",
   "metadata": {},
   "source": [
    "## Coding Models For Multilingal Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1904a1b",
   "metadata": {},
   "source": [
    "**Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96bd8574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61fc6a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-openai langchain-mistralai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8b7fcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# loads the .env file with LLM API Keys\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dad380b8-93ca-4cd2-b0c0-4799aab72047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets API access keys\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd3e00",
   "metadata": {},
   "source": [
    "#### Creating OpenAI LangChain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc669a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "open_ai_gpt_4o_mini = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    api_key=OPEN_AI_API_KEY\n",
    ")\n",
    "\n",
    "# establish a system prompt with a test message\n",
    "\n",
    "open_ai_test_messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        You're a helpful assistant, you can read english tweets and determine the writers sentiment.\n",
    "\n",
    "        After reading a tweet, and carefully determining it's sentiment, you respond with either 0: for positive sentiment, or 1 for negative sentiment\n",
    "        \"\"\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f40e72",
   "metadata": {},
   "source": [
    "**Test LangChain OpenAI Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test message\n",
    "\n",
    "test_positive_prompt = (\"human\", \"I love programming large languages models at PSU! :)\")\n",
    "\n",
    "# add the positive\n",
    "open_ai_test_messages.append(test_positive_prompt)\n",
    "\n",
    "\n",
    "# attempt to get a response from the OpenAI model\n",
    "open_ai_message = open_ai_gpt_4o_mini.invoke(open_ai_test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65091659",
   "metadata": {},
   "source": [
    "**Print Open AI model's Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "116a3d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(open_ai_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166790d",
   "metadata": {},
   "source": [
    "#### Creating Mistral Nemo LangChain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "mistral_nemo = ChatMistralAI(\n",
    "    model=\"open-mistral-nemo\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    api_key=MISTRAL_API_KEY\n",
    ")\n",
    "\n",
    "nemo_test_messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        You're a helpful assistant, you can read english tweets and determine the writers sentiment.\n",
    "\n",
    "        After reading a tweet, and carefully determining it's sentiment, you respond with either 0: for positive sentiment, or 1 for negative sentiment\n",
    "        \"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89db7d",
   "metadata": {},
   "source": [
    "**Test LangChain Mistral Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "956f0827",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPStatusError",
     "evalue": "Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {\"object\":\"error\",\"message\":\"Invalid model: open-mistal-nemo\",\"type\":\"invalid_model\",\"param\":null,\"code\":\"1500\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m nemo_test_messages\u001b[38;5;241m.\u001b[39mappend(test_positive_prompt)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# invoke the model to get a response\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model_resp \u001b[38;5;241m=\u001b[39m \u001b[43mmistral_nemo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnemo_test_messages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-3/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-3/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-3/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-3/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-3/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-3/.venv/lib/python3.12/site-packages/langchain_mistralai/chat_models.py:543\u001b[0m, in \u001b[0;36mChatMistralAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    542\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 543\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-3/.venv/lib/python3.12/site-packages/langchain_mistralai/chat_models.py:462\u001b[0m, in \u001b[0;36mChatMistralAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m         _raise_on_error(response)\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m--> 462\u001b[0m rtn \u001b[38;5;241m=\u001b[39m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rtn\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-3/.venv/lib/python3.12/site-packages/langchain_mistralai/chat_models.py:459\u001b[0m, in \u001b[0;36mChatMistralAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 459\u001b[0m     \u001b[43m_raise_on_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-3/.venv/lib/python3.12/site-packages/langchain_mistralai/chat_models.py:170\u001b[0m, in \u001b[0;36m_raise_on_error\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mis_error(response\u001b[38;5;241m.\u001b[39mstatus_code):\n\u001b[1;32m    169\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError(\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError response \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile fetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    173\u001b[0m         request\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[1;32m    174\u001b[0m         response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    175\u001b[0m     )\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {\"object\":\"error\",\"message\":\"Invalid model: open-mistal-nemo\",\"type\":\"invalid_model\",\"param\":null,\"code\":\"1500\"}"
     ]
    }
   ],
   "source": [
    "test_positive_prompt = (\"human\", \"I love programming large language models at PSU :)\")\n",
    "\n",
    "# append the positive tweet\n",
    "nemo_test_messages.append(test_positive_prompt)\n",
    "\n",
    "# invoke the model to get a response\n",
    "model_resp = mistral_nemo.invoke(nemo_test_messages)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
