{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be002d99-2463-46c2-9cc8-2236e78c1e0d",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c6a29-0554-41c6-ba2e-88714b5a40e0",
   "metadata": {},
   "source": [
    "## Load The Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdfa71-edf1-462c-8dd7-08196076f4f9",
   "metadata": {},
   "source": [
    "**Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fcd557-789a-4eb6-918e-5733571739b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.11.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8349e6-ad8f-40ca-a710-564335aead49",
   "metadata": {},
   "source": [
    "### Load French Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa51284-87ac-4731-a200-64e9b3afa0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregwitt/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-3/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    RT @user: Le piège de l’#énergie verte (via @user) http\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# French Validation \n",
    "ds_validation_french = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"french\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_french } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_french['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_french['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9719fa5f",
   "metadata": {},
   "source": [
    "### Split French Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d664a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative french tweets: 108\n",
      "number of positive french tweets: 108\n",
      "\n",
      "Random negative french tweet: Manifestation contre les éoliennes à Foix http\n",
      "random positive french tweet: Mon TRÈS CHER LOUP 400 loups qui coûtent 12 millions d'€ d'indemnité = 30000€ par loup ! Merci les écologistes\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_french_tweets = ds_validation_french.filter(lambda x: x['label'] == 0)\n",
    "positive_french_tweets = ds_validation_french.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative french tweets: {len(negative_french_tweets)}\")\n",
    "print(f\"number of positive french tweets: {len(positive_french_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative french tweet: {negative_french_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive french tweet: {positive_french_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be09296e-705f-4664-b1ce-52e2c1a766ba",
   "metadata": {},
   "source": [
    "### Load German Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a4078a-5a49-4861-8c6a-33da59e3ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    Beim niesen werden alle werden alle Körperfunkionen ausgesetzt sogar das Herz\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# German Validation Tweets\n",
    "ds_validation_german = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"german\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_german } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,324)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_german['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_german['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d21a17",
   "metadata": {},
   "source": [
    "### Split German Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2092cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative german tweets: 108\n",
      "number of positive german tweets: 108\n",
      "\n",
      "Random negative german tweet: Meine ganzen Runtastic Apps wurden nicht wiederhergestellt?!\n",
      "random positive german tweet: RT @user: TTTTTTOOOOOOORRRRR durch Ronny! Nach einer klasse Einzelleistung trifft Herthas Nummer 12 zum 6:1 gegen Eintracht Frankfurt.…\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_german_tweets = ds_validation_german.filter(lambda x: x['label'] == 0)\n",
    "positive_german_tweets = ds_validation_german.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative german tweets: {len(negative_german_tweets)}\")\n",
    "print(f\"number of positive german tweets: {len(positive_german_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative german tweet: {negative_german_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive german tweet: {positive_german_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f504a2b-a047-476e-acf5-1a59e6c3fb0d",
   "metadata": {},
   "source": [
    "### Load Hindi Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eacde19e-108e-4dda-a3f8-82f94b589bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    match hrane me guddu ka maa ka bahut bada hath hota agr guddu ke papa rokte nhi  .  .  . \n",
      "\n",
      "    Label: \n",
      "    2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Hindi Validation Tweets\n",
    "ds_validation_hindi = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"hindi\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_hindi } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_hindi['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_hindi['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35125781",
   "metadata": {},
   "source": [
    "### Split Hindi Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d2936a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative hindi tweets: 108\n",
      "number of positive hindi tweets: 108\n",
      "\n",
      "Random negative hindi tweet: awesome hillarious or no words can describe this great comedy\n",
      "random positive hindi tweet: victor cruz isnt himself today .  .  .  .  .  .  .  . i know he'll get it together in the 2nd half\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_hindi_tweets = ds_validation_hindi.filter(lambda x: x['label'] == 0)\n",
    "positive_hindi_tweets = ds_validation_hindi.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative hindi tweets: {len(negative_hindi_tweets)}\")\n",
    "print(f\"number of positive hindi tweets: {len(positive_hindi_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative hindi tweet: {negative_hindi_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive hindi tweet: {positive_hindi_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005eba5-5c2b-437f-932a-8dca157ba6f2",
   "metadata": {},
   "source": [
    "### Load Italian Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "359c8a26-4340-4c35-891e-ab80a6fd1ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    Province: dal vertice fiorentino dei \"dieci\" parte la proposta al governo Monti http #o3news\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# German Validation Tweets\n",
    "ds_validation_italian = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"italian\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_italian } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_italian['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_italian['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe8b7a",
   "metadata": {},
   "source": [
    "### Split Italian Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f538fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative italian tweets: 108\n",
      "number of positive italian tweets: 108\n",
      "\n",
      "Random negative italian tweet: Che poi, a guardare bene, l'unica differenza è che #Grillo è delNord, e #Cettolaqualunque del sud. Suv compresi.\n",
      "random positive italian tweet: @user anche a me piace la tua! :))\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_italian_tweets = ds_validation_italian.filter(lambda x: x['label'] == 0)\n",
    "positive_italian_tweets = ds_validation_italian.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative italian tweets: {len(negative_italian_tweets)}\")\n",
    "print(f\"number of positive italian tweets: {len(positive_italian_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative italian tweet: {negative_italian_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive italian tweet: {positive_italian_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06368255-4405-4cf6-a197-4cec8e748c0d",
   "metadata": {},
   "source": [
    "### Load Portuguese Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50e5e408-db8b-4974-bcaa-a11f5a1daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    Carol 😅😅😅😅😅 homens lavem a louca #Encontro\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Portuguese Validation Tweets\n",
    "ds_validation_portuguese = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"portuguese\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_german } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_portuguese['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_portuguese['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfc6f7",
   "metadata": {},
   "source": [
    "### Split Portuguese Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "681dffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative portuguese tweets: 108\n",
      "number of positive portuguese tweets: 108\n",
      "\n",
      "Random negative portuguese tweet: o programa acabou e ninguém tomou um tacacá #MasterChefBR\n",
      "random positive portuguese tweet: O samba é resistência sem fim. Que aula! #ConversacomBial\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_portuguese_tweets = ds_validation_portuguese.filter(lambda x: x['label'] == 0)\n",
    "positive_portuguese_tweets = ds_validation_portuguese.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative portuguese tweets: {len(negative_portuguese_tweets)}\")\n",
    "print(f\"number of positive portuguese tweets: {len(positive_portuguese_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative portuguese tweet: {negative_portuguese_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive portuguese tweet: {positive_portuguese_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a111e8-1d28-46d1-81c3-0b41a06a2273",
   "metadata": {},
   "source": [
    "### Load Spanish Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed089dbb-243e-4d82-b675-77dd2e61bb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    @user Bueno, yo soy de la Real, neutral total\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Portuguese Validation Tweets\n",
    "ds_validation_spanish = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"spanish\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_spanish } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_spanish['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_spanish['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151163f7-4392-4953-9e3e-27debd553776",
   "metadata": {},
   "source": [
    "### Split Spanish Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b4c94-32a6-42b1-82f3-4c463849edbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative spanish tweets: 108\n",
      "number of positive spanish tweets: 108\n",
      "\n",
      "random negative spanish tweet: Me habría gustado hacer alguna de las de los Targaryen pero como están  en paradero desconocido pueh no hay imagenes buenas de ellas y\n",
      "random positive spanish tweet: No es triste, es normal\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_spanish_tweets = ds_validation_spanish.filter(lambda x: x['label'] == 0)\n",
    "positive_spanish_tweets = ds_validation_spanish.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative spanish tweets: {len(negative_spanish_tweets)}\")\n",
    "print(f\"number of positive spanish tweets: {len(positive_spanish_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"random negative spanish tweet: {negative_spanish_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive spanish tweet: {positive_spanish_tweets['text'][random_num]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccddf5d",
   "metadata": {},
   "source": [
    "## Multilingual Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6694864a",
   "metadata": {},
   "source": [
    "### Chosen Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383427ef",
   "metadata": {},
   "source": [
    "### OpenAI GPT 4o - Mini \n",
    "\n",
    "[4o Mini Documentation](https://platform.openai.com/docs/models#gpt-4o-mini)   \n",
    "[GPT 4o Model Card](https://openai.com/index/gpt-4o-system-card/)  \n",
    "[Scoring Evaluation Metrics](https://deepgram.com/learn/arc-llm-benchmark-guide)  \n",
    "[LangChain OpenAI Chat Models](https://python.langchain.com/docs/integrations/chat/openai/)\n",
    "\n",
    "**Multilingual Capabilities**\n",
    "\n",
    "According to the documentation the GPT-4 Models are high performance models and as of 2023, with roughly ~8B parameters. It the state of the art flag ship model for their market for small projects. The improved accuracy in other smaller non-english languages has been a company claim based on the documentation the model's metrics are represented below with ARC-Easy scores, and TruthfulQA scores\n",
    "\n",
    "**GPT ARC Easy Language Scores**\n",
    "\n",
    "![GPT ARC Easy Language Scores](./img/GPT-ARC-Easy-Score.png)\n",
    "\n",
    "**GPT TruthfulQA Scores**\n",
    "\n",
    "![GPT TruthfulQA Scores](./img/GPT-TruthfulQA-Score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e12834",
   "metadata": {},
   "source": [
    "### MistralAI NeMo Model\n",
    "\n",
    "[NeMo Model Documentation](https://mistral.ai/news/mistral-nemo/)  \n",
    "[Other Mistral Models](https://docs.mistral.ai/getting-started/models/models_overview/)  \n",
    "[LangChain Mistral Chat Model ](https://python.langchain.com/docs/integrations/chat/mistralai/#invocation)\n",
    "\n",
    "\n",
    "\n",
    "Mistral Nemo is the best small model, state of the art model with 12B. The Model is *\"designed for multilingual applications\"* it's training is explicitly mentioned in the above article for *\"English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi*. The Mistral documentation even boasts about a new tokenizer used for this model specifically **Tekken** which outperformed it's previous models by ~30%. Compared to Llama 3's tokenizer, NeMo's *\"proved more proficient in compressing text for approcimately 85% of all languages.\"*\n",
    "\n",
    "**Tekken Compression**\n",
    "\n",
    "![Tekken Compression](./img/Tekken-Compression-Rate.png)\n",
    "\n",
    "**NeMo Language Metrics**\n",
    "\n",
    "![Language Performance Metrics](./img/Mistral-Nemo-Language-Scores.png)\n",
    "\n",
    "**Open Source Model Performance Metrics**\n",
    "\n",
    "![Mistral Nemo 12B](./img/MIstral-Nemo-12B-Metrics.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a65fe8",
   "metadata": {},
   "source": [
    "## Coding Models For Multilingal Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1904a1b",
   "metadata": {},
   "source": [
    "**Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96bd8574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "61fc6a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-openai langchain-mistralai langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8b7fcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# loads the .env file with LLM API Keys\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dad380b8-93ca-4cd2-b0c0-4799aab72047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets API access keys\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd3e00",
   "metadata": {},
   "source": [
    "#### Creating OpenAI LangChain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc669a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "open_ai_gpt_4o_mini = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    api_key=OPEN_AI_API_KEY\n",
    ")\n",
    "\n",
    "# establish a system prompt with a test message\n",
    "\n",
    "open_ai_test_messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        You're a helpful assistant, you can read english tweets and determine the writers sentiment.\n",
    "\n",
    "        After reading a tweet, and carefully determining it's sentiment, you respond with either 0: for positive sentiment, or 1 for negative sentiment\n",
    "        \"\"\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f40e72",
   "metadata": {},
   "source": [
    "**Test LangChain OpenAI Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test message\n",
    "\n",
    "test_positive_prompt = (\"human\", \"I love programming large languages models at PSU! :)\")\n",
    "\n",
    "# add the positive\n",
    "open_ai_test_messages.append(test_positive_prompt)\n",
    "\n",
    "\n",
    "# attempt to get a response from the OpenAI model\n",
    "open_ai_message = open_ai_gpt_4o_mini.invoke(open_ai_test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65091659",
   "metadata": {},
   "source": [
    "**Determine Open AI model's Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "116a3d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(open_ai_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166790d",
   "metadata": {},
   "source": [
    "#### Creating Mistral Nemo LangChain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9bc6c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "mistral_nemo = ChatMistralAI(\n",
    "    model=\"open-mistral-nemo\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    api_key=MISTRAL_API_KEY\n",
    ")\n",
    "\n",
    "nemo_test_messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        You're a helpful assistant, you can read english tweets and determine the writers sentiment.\n",
    "\n",
    "        After reading a tweet, and carefully determining it's sentiment, you respond with either 0: for positive sentiment, or 1 for negative sentiment\n",
    "        \"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89db7d",
   "metadata": {},
   "source": [
    "**Test LangChain Mistral Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "956f0827",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_positive_prompt = (\"human\", \"I love programming large language models at PSU :)\")\n",
    "\n",
    "# append the positive tweet\n",
    "nemo_test_messages.append(test_positive_prompt)\n",
    "\n",
    "# invoke the model to get a response\n",
    "model_resp = mistral_nemo.invoke(nemo_test_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0177e295",
   "metadata": {},
   "source": [
    "**Determine Mistral Model's Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "51608997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(model_resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c597a93",
   "metadata": {},
   "source": [
    "**Build Randomizer Function for Sentiment Analysis**\n",
    "\n",
    "the `randomize_tweets` method conveniently leverages two methods from huggingface's `datasets` library to accomplish the task of *shuffling* and *combining* tweets of any language after they have been filtered in the previous section. By using [datasets.shuffle](https://huggingface.co/docs/datasets/v3.1.0/en/package_reference/main_classes#datasets.Dataset.shuffle) and also [datasets.concatenate_datasets](https://huggingface.co/docs/datasets/v3.1.0/en/package_reference/main_classes#datasets.concatenate_datasets) the incoming positive tweets and negative tweets are returned as a shuffled dataset that can be supplied to our chain for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b7d15980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def randomize_tweets(positive_tweets, negative_tweets):\n",
    "    # combine the positive \n",
    "    positive_negative_tweets = concatenate_datasets([positive_tweets, negative_tweets])\n",
    "    # shuffle dataset\n",
    "    shuffled_tweets = positive_negative_tweets.shuffle(positive_negative_tweets)\n",
    "    return shuffled_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a5c6f",
   "metadata": {},
   "source": [
    "**Building a LangChain Prompt Template**\n",
    "\n",
    "The process of building [prompt_templates](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) with LangChain is fairly easy for a simple prompt like the ones needed for this task. They can grow exponentially with chaining and using LangChains `LangChain Expression Language` or `LCEL`. below we will build a simple template that will be leveraged for the Multi-Language Sentiment task. For this part it's easy to reuse the same technique we used before for the system prompt in the initial test of the langchain models for both OpenAI and Mistral. This also, conveniently ensures some amount of consistency for both models. simply passing a `{language}` and a `{tweet}` is all that is needed. This will all come together when we build our [chain](https://python.langchain.com/docs/how_to/sequence/) to invoke when both using the prompts, in the form of tweets and the llm to produce a response we can measure against our ground truth. the invocation will look something like the implementation below:\n",
    "\n",
    "```python\n",
    "# tweet chain's invoke method will take arguments as a dict: { \"language\": <language>, \"tweet\": <tweet>}\n",
    "# this will chain the response of the build ChatTemplate to the next element in the chain the llm\n",
    "# then the response will get chained to the StrOutoutParser() for evaluation\n",
    "tweet_sentiment_chain = prompt_template | llm | StrOutputParser()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3416db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"\"\"\n",
    "        You're a helpful assistant, you can read {language}, and english tweets and determine the writers sentiment.\n",
    "\n",
    "        After reading a tweet, and carefully determining it's sentiment, you respond with either 0: for positive sentiment, or 1 for negative sentiment\n",
    "        \"\"\"),\n",
    "    (\"human\", \"{tweet}: \")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f053d",
   "metadata": {},
   "source": [
    "#### Test the ChatPromptTemplate with Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c0818",
   "metadata": {},
   "source": [
    "**OpenAI GPT 4o mini test using ChatPromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d1354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# build the open_ai_4o_mini chain\n",
    "open_ai_tweet_sentiment_chain = prompt_template | open_ai_gpt_4o_mini | StrOutputParser()\n",
    "\n",
    "# invoke the chain with a test tweet \n",
    "\n",
    "# spanish_tweet:  \"Estoy triste porque no puedo hablar español con fluidez.\"\n",
    "# translation: \"I am sad I can't speak spanish fluently\"\n",
    "open_ai_tweet_sentiment_chain.invoke({\"language\": \"spanish\", \"tweet\": \"Estoy triste porque no puedo hablar español con fluidez.\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae97f59",
   "metadata": {},
   "source": [
    "**Mistral Nemo test using the ChatPromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ac302a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# build the open_ai_4o_mini chain\n",
    "mistral_nemo_tweet_sentiment_chain = prompt_template | mistral_nemo | StrOutputParser()\n",
    "\n",
    "# invoke the chain with a test tweet \n",
    "\n",
    "# spanish_tweet:  \"Estoy triste porque no puedo hablar español con fluidez.\"\n",
    "# translation: \"I am sad I can't speak spanish fluently\"\n",
    "mistral_nemo_tweet_sentiment_chain.invoke({\"language\": \"spanish\", \"tweet\": \"Estoy triste porque no puedo hablar español con fluidez.\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
