{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be002d99-2463-46c2-9cc8-2236e78c1e0d",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c6a29-0554-41c6-ba2e-88714b5a40e0",
   "metadata": {},
   "source": [
    "## Load The Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdfa71-edf1-462c-8dd7-08196076f4f9",
   "metadata": {},
   "source": [
    "**Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fcd557-789a-4eb6-918e-5733571739b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.11.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8349e6-ad8f-40ca-a710-564335aead49",
   "metadata": {},
   "source": [
    "### Load French Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa51284-87ac-4731-a200-64e9b3afa0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregwitt/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-3/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    RT @user: Le piège de l’#énergie verte (via @user) http\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# French Validation \n",
    "ds_validation_french = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"french\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_french } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_french['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_french['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9719fa5f",
   "metadata": {},
   "source": [
    "### Split French Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d664a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative french tweets: 108\n",
      "number of positive french tweets: 108\n",
      "\n",
      "Random negative french tweet: Manifestation contre les éoliennes à Foix http\n",
      "random positive french tweet: Mon TRÈS CHER LOUP 400 loups qui coûtent 12 millions d'€ d'indemnité = 30000€ par loup ! Merci les écologistes\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_french_tweets = ds_validation_french.filter(lambda x: x['label'] == 0)\n",
    "positive_french_tweets = ds_validation_french.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative french tweets: {len(negative_french_tweets)}\")\n",
    "print(f\"number of positive french tweets: {len(positive_french_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative french tweet: {negative_french_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive french tweet: {positive_french_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be09296e-705f-4664-b1ce-52e2c1a766ba",
   "metadata": {},
   "source": [
    "### Load German Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a4078a-5a49-4861-8c6a-33da59e3ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    Beim niesen werden alle werden alle Körperfunkionen ausgesetzt sogar das Herz\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# German Validation Tweets\n",
    "ds_validation_german = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"german\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_german } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,324)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_german['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_german['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d21a17",
   "metadata": {},
   "source": [
    "### Split German Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2092cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative german tweets: 108\n",
      "number of positive german tweets: 108\n",
      "\n",
      "Random negative german tweet: Meine ganzen Runtastic Apps wurden nicht wiederhergestellt?!\n",
      "random positive german tweet: RT @user: TTTTTTOOOOOOORRRRR durch Ronny! Nach einer klasse Einzelleistung trifft Herthas Nummer 12 zum 6:1 gegen Eintracht Frankfurt.…\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_german_tweets = ds_validation_german.filter(lambda x: x['label'] == 0)\n",
    "positive_german_tweets = ds_validation_german.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative german tweets: {len(negative_german_tweets)}\")\n",
    "print(f\"number of positive german tweets: {len(positive_german_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative german tweet: {negative_german_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive german tweet: {positive_german_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f504a2b-a047-476e-acf5-1a59e6c3fb0d",
   "metadata": {},
   "source": [
    "### Load Hindi Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eacde19e-108e-4dda-a3f8-82f94b589bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    match hrane me guddu ka maa ka bahut bada hath hota agr guddu ke papa rokte nhi  .  .  . \n",
      "\n",
      "    Label: \n",
      "    2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Hindi Validation Tweets\n",
    "ds_validation_hindi = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"hindi\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_hindi } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_hindi['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_hindi['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35125781",
   "metadata": {},
   "source": [
    "### Split Hindi Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d2936a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative hindi tweets: 108\n",
      "number of positive hindi tweets: 108\n",
      "\n",
      "Random negative hindi tweet: awesome hillarious or no words can describe this great comedy\n",
      "random positive hindi tweet: victor cruz isnt himself today .  .  .  .  .  .  .  . i know he'll get it together in the 2nd half\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_hindi_tweets = ds_validation_hindi.filter(lambda x: x['label'] == 0)\n",
    "positive_hindi_tweets = ds_validation_hindi.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative hindi tweets: {len(negative_hindi_tweets)}\")\n",
    "print(f\"number of positive hindi tweets: {len(positive_hindi_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative hindi tweet: {negative_hindi_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive hindi tweet: {positive_hindi_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005eba5-5c2b-437f-932a-8dca157ba6f2",
   "metadata": {},
   "source": [
    "### Load Italian Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "359c8a26-4340-4c35-891e-ab80a6fd1ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    Province: dal vertice fiorentino dei \"dieci\" parte la proposta al governo Monti http #o3news\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# German Validation Tweets\n",
    "ds_validation_italian = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"italian\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_italian } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_italian['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_italian['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe8b7a",
   "metadata": {},
   "source": [
    "### Split Italian Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f538fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative italian tweets: 108\n",
      "number of positive italian tweets: 108\n",
      "\n",
      "Random negative italian tweet: Che poi, a guardare bene, l'unica differenza è che #Grillo è delNord, e #Cettolaqualunque del sud. Suv compresi.\n",
      "random positive italian tweet: @user anche a me piace la tua! :))\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_italian_tweets = ds_validation_italian.filter(lambda x: x['label'] == 0)\n",
    "positive_italian_tweets = ds_validation_italian.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative italian tweets: {len(negative_italian_tweets)}\")\n",
    "print(f\"number of positive italian tweets: {len(positive_italian_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative italian tweet: {negative_italian_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive italian tweet: {positive_italian_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06368255-4405-4cf6-a197-4cec8e748c0d",
   "metadata": {},
   "source": [
    "### Load Portuguese Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50e5e408-db8b-4974-bcaa-a11f5a1daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    Carol 😅😅😅😅😅 homens lavem a louca #Encontro\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Portuguese Validation Tweets\n",
    "ds_validation_portuguese = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"portuguese\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_german } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_portuguese['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_portuguese['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfc6f7",
   "metadata": {},
   "source": [
    "### Split Portuguese Tweets By Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "681dffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative portuguese tweets: 108\n",
      "number of positive portuguese tweets: 108\n",
      "\n",
      "Random negative portuguese tweet: o programa acabou e ninguém tomou um tacacá #MasterChefBR\n",
      "random positive portuguese tweet: O samba é resistência sem fim. Que aula! #ConversacomBial\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_portuguese_tweets = ds_validation_portuguese.filter(lambda x: x['label'] == 0)\n",
    "positive_portuguese_tweets = ds_validation_portuguese.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative portuguese tweets: {len(negative_portuguese_tweets)}\")\n",
    "print(f\"number of positive portuguese tweets: {len(positive_portuguese_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"Random negative portuguese tweet: {negative_portuguese_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive portuguese tweet: {positive_portuguese_tweets['text'][random_num]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a111e8-1d28-46d1-81c3-0b41a06a2273",
   "metadata": {},
   "source": [
    "### Load Spanish Language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed089dbb-243e-4d82-b675-77dd2e61bb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    @user Bueno, yo soy de la Real, neutral total\n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Portuguese Validation Tweets\n",
    "ds_validation_spanish = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"spanish\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_validation_spanish } \n",
    "    \"\"\")\n",
    "\n",
    "random_tweet_index = random.randint(0,323)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation_spanish['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation_spanish['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151163f7-4392-4953-9e3e-27debd553776",
   "metadata": {},
   "source": [
    "### Split Spanish Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b4c94-32a6-42b1-82f3-4c463849edbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative spanish tweets: 108\n",
      "number of positive spanish tweets: 108\n",
      "\n",
      "random negative spanish tweet: Me habría gustado hacer alguna de las de los Targaryen pero como están  en paradero desconocido pueh no hay imagenes buenas de ellas y\n",
      "random positive spanish tweet: No es triste, es normal\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# sets a for consistent repeatability amongst the tests\n",
    "random.seed(23)\n",
    "\n",
    "# filter the data for each type of tweet. \n",
    "# filter is specific to hugging face datasets \n",
    "# it will take a lambda that will return a boolean value to filter elements on\n",
    "\n",
    "negative_spanish_tweets = ds_validation_spanish.filter(lambda x: x['label'] == 0)\n",
    "positive_spanish_tweets = ds_validation_spanish.filter(lambda x: x['label'] == 2)\n",
    "\n",
    "# Optional: Preview the sizes\n",
    "print(f\"number of negative spanish tweets: {len(negative_spanish_tweets)}\")\n",
    "print(f\"number of positive spanish tweets: {len(positive_spanish_tweets)}\")\n",
    "\n",
    "print()\n",
    "random_num = random.randint(0,107)\n",
    "\n",
    "print(f\"random negative spanish tweet: {negative_spanish_tweets['text'][random_num]}\")\n",
    "\n",
    "print(f\"random positive spanish tweet: {positive_spanish_tweets['text'][random_num]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccddf5d",
   "metadata": {},
   "source": [
    "## Multilingual Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6694864a",
   "metadata": {},
   "source": [
    "### Chosen Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383427ef",
   "metadata": {},
   "source": [
    "### OpenAI GPT 4o - Mini \n",
    "\n",
    "[4o Mini Documentation](https://platform.openai.com/docs/models#gpt-4o-mini)   \n",
    "[GPT 4o Model Card](https://openai.com/index/gpt-4o-system-card/)  \n",
    "[Scoring Evaluation Metrics](https://deepgram.com/learn/arc-llm-benchmark-guide)  \n",
    "[LangChain OpenAI Chat Models](https://python.langchain.com/docs/integrations/chat/openai/)\n",
    "\n",
    "**Multilingual Capabilities**\n",
    "\n",
    "According to the documentation the GPT-4 Models are high performance models and as of 2023, with roughly ~8B parameters. It the state of the art flag ship model for their market for small projects. The improved accuracy in other smaller non-english languages has been a company claim based on the documentation the model's metrics are represented below with ARC-Easy scores, and TruthfulQA scores\n",
    "\n",
    "**GPT ARC Easy Language Scores**\n",
    "\n",
    "![GPT ARC Easy Language Scores](./img/GPT-ARC-Easy-Score.png)\n",
    "\n",
    "**GPT TruthfulQA Scores**\n",
    "\n",
    "![GPT TruthfulQA Scores](./img/GPT-TruthfulQA-Score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e12834",
   "metadata": {},
   "source": [
    "### MistralAI NeMo Model\n",
    "\n",
    "[NeMo Model Documentation](https://mistral.ai/news/mistral-nemo/)  \n",
    "[Other Mistral Models](https://docs.mistral.ai/getting-started/models/models_overview/)  \n",
    "[LangChain Mistral Chat Model ](https://python.langchain.com/docs/integrations/chat/mistralai/#invocation)\n",
    "\n",
    "\n",
    "\n",
    "Mistral Nemo is the best small model, state of the art model with 12B. The Model is *\"designed for multilingual applications\"* it's training is explicitly mentioned in the above article for *\"English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi*. The Mistral documentation even boasts about a new tokenizer used for this model specifically **Tekken** which outperformed it's previous models by ~30%. Compared to Llama 3's tokenizer, NeMo's *\"proved more proficient in compressing text for approcimately 85% of all languages.\"*\n",
    "\n",
    "**Tekken Compression**\n",
    "\n",
    "![Tekken Compression](./img/Tekken-Compression-Rate.png)\n",
    "\n",
    "**NeMo Language Metrics**\n",
    "\n",
    "![Language Performance Metrics](./img/Mistral-Nemo-Language-Scores.png)\n",
    "\n",
    "**Open Source Model Performance Metrics**\n",
    "\n",
    "![Mistral Nemo 12B](./img/MIstral-Nemo-12B-Metrics.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a65fe8",
   "metadata": {},
   "source": [
    "## Coding Models For Multilingal Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1904a1b",
   "metadata": {},
   "source": [
    "**Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96bd8574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "61fc6a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-openai langchain-mistralai langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8b7fcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# loads the .env file with LLM API Keys\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dad380b8-93ca-4cd2-b0c0-4799aab72047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets API access keys\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd3e00",
   "metadata": {},
   "source": [
    "#### Creating OpenAI LangChain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc669a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "open_ai_gpt_4o_mini = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    api_key=OPEN_AI_API_KEY\n",
    ")\n",
    "\n",
    "# establish a system prompt with a test message\n",
    "\n",
    "open_ai_test_messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        You're a helpful assistant, you can read english tweets and determine the writers sentiment.\n",
    "\n",
    "        After reading a tweet, and carefully determining it's sentiment, you respond with either 0: for positive sentiment, or 1 for negative sentiment\n",
    "        \"\"\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f40e72",
   "metadata": {},
   "source": [
    "**Test LangChain OpenAI Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test message\n",
    "\n",
    "test_positive_prompt = (\"human\", \"I love programming large languages models at PSU! :)\")\n",
    "\n",
    "# add the positive\n",
    "open_ai_test_messages.append(test_positive_prompt)\n",
    "\n",
    "\n",
    "# attempt to get a response from the OpenAI model\n",
    "open_ai_message = open_ai_gpt_4o_mini.invoke(open_ai_test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65091659",
   "metadata": {},
   "source": [
    "**Determine Open AI model's Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "116a3d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(open_ai_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166790d",
   "metadata": {},
   "source": [
    "#### Creating Mistral Nemo LangChain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "mistral_nemo = ChatMistralAI(\n",
    "    model=\"open-mistral-nemo\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    api_key=MISTRAL_API_KEY\n",
    ")\n",
    "\n",
    "nemo_test_messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        You're a helpful assistant, you can read english tweets and determine the writers sentiment.\n",
    "\n",
    "        After reading a tweet, and carefully determining it's sentiment, you respond with either 0: for positive sentiment, or 1 for negative sentiment\n",
    "        \"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89db7d",
   "metadata": {},
   "source": [
    "**Test LangChain Mistral Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "956f0827",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_positive_prompt = (\"human\", \"I love programming large language models at PSU :)\")\n",
    "\n",
    "# append the positive tweet\n",
    "nemo_test_messages.append(test_positive_prompt)\n",
    "\n",
    "# invoke the model to get a response\n",
    "model_resp = mistral_nemo.invoke(nemo_test_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0177e295",
   "metadata": {},
   "source": [
    "**Determine Mistral Model's Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "51608997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(model_resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c597a93",
   "metadata": {},
   "source": [
    "**Build Randomizer Function for Sentiment Analysis**\n",
    "\n",
    "the `randomize_tweets` method conveniently leverages two methods from huggingface's `datasets` library to accomplish the task of *shuffling* and *combining* tweets of any language after they have been filtered in the previous section. By using [datasets.shuffle](https://huggingface.co/docs/datasets/v3.1.0/en/package_reference/main_classes#datasets.Dataset.shuffle) and also [datasets.concatenate_datasets](https://huggingface.co/docs/datasets/v3.1.0/en/package_reference/main_classes#datasets.concatenate_datasets) the incoming positive tweets and negative tweets are returned as a shuffled dataset that can be supplied to our chain for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b7d15980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def randomize_tweets(positive_tweets, negative_tweets):\n",
    "    # combine the positive \n",
    "    positive_negative_tweets = concatenate_datasets([positive_tweets, negative_tweets])\n",
    "    # shuffle dataset\n",
    "    shuffled_tweets = positive_negative_tweets.shuffle(positive_negative_tweets)\n",
    "    return shuffled_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a5c6f",
   "metadata": {},
   "source": [
    "**Building a LangChain Prompt Template**\n",
    "\n",
    "The process of building [prompt_templates](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) with LangChain is fairly easy for a simple prompt like the ones needed for this task. They can grow exponentially with chaining and using LangChains `LangChain Expression Language` or `LCEL`. below we will build a simple template that will be leveraged for the Multi-Language Sentiment task. For this part it's easy to reuse the same technique we used before for the system prompt in the initial test of the langchain models for both OpenAI and Mistral. This also, conveniently ensures some amount of consistency for both models. simply passing a `{language}` and a `{tweet}` is all that is needed. This will all come together when we build our [chain](https://python.langchain.com/docs/how_to/sequence/) to invoke when both using the prompts, in the form of tweets and the llm to produce a response we can measure against our ground truth. the invocation will look something like the implementation below:\n",
    "\n",
    "```python\n",
    "# tweet chain's invoke method will take arguments as a dict: { \"language\": <language>, \"tweet\": <tweet>}\n",
    "# this will chain the response of the build ChatTemplate to the next element in the chain the llm\n",
    "# then the response will get chained to the StrOutoutParser() for evaluation\n",
    "tweet_sentiment_chain = prompt_template | llm | StrOutputParser()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3416db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "44b5e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"\"\"\n",
    "        You're a helpful assistant, you can read {language}, and english tweets and determine the writers sentiment.\n",
    "\n",
    "        After reading a tweet, and carefully determining it's sentiment, you respond with ONLY: \n",
    "        \n",
    "        0: for positive sentiment\n",
    "        1: for negative sentiment\n",
    "        \"\"\"),\n",
    "    (\"human\", \"{tweet}: \")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f053d",
   "metadata": {},
   "source": [
    "#### Test the ChatPromptTemplate with Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c0818",
   "metadata": {},
   "source": [
    "**OpenAI GPT 4o mini test using ChatPromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "139d1354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# build the open_ai_4o_mini chain\n",
    "open_ai_tweet_sentiment_chain = prompt_template | open_ai_gpt_4o_mini | StrOutputParser()\n",
    "\n",
    "# invoke the chain with a test tweet \n",
    "\n",
    "# spanish_tweet:  \"Estoy triste porque no puedo hablar español con fluidez.\"\n",
    "# translation: \"I am sad I can't speak spanish fluently\"\n",
    "open_ai_tweet_sentiment_chain.invoke({\"language\": \"spanish\", \"tweet\": \"Estoy triste porque no puedo hablar español con fluidez.\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae97f59",
   "metadata": {},
   "source": [
    "**Mistral Nemo test using the ChatPromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ac302a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# build the open_ai_4o_mini chain\n",
    "mistral_nemo_tweet_sentiment_chain = prompt_template | mistral_nemo | StrOutputParser()\n",
    "\n",
    "# invoke the chain with a test tweet \n",
    "\n",
    "# spanish_tweet:  \"Estoy triste porque no puedo hablar español con fluidez.\"\n",
    "# translation: \"I am sad I can't speak spanish fluently\"\n",
    "mistral_nemo_tweet_sentiment_chain.invoke({\"language\": \"spanish\", \"tweet\": \"Estoy triste porque no puedo hablar español con fluidez.\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa143fe5",
   "metadata": {},
   "source": [
    "### Native Language Chaining for Model Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53e06c",
   "metadata": {},
   "source": [
    "the success of the last implementation is promising for this technique to be efficient to handle the english prompting but what about the potential for the native language speakers using these large language models?\n",
    "\n",
    "to simulate this, I have built native language `ChatPromptTemplates` I used the models in question to produce the native translation using the english prompt as a starting point. For example if I was testing against *OpenAI's 4o Mini* I will use `ChatGPT` to produce a translation for the system prompt in french from the same prompt I used in english:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "You're a helpful assistant, you can read french, and english tweets and determine the writers sentiment.\n",
    "\n",
    "After reading a tweet, and carefully determining it's sentiment, you respond with ONLY: \n",
    "        \n",
    "0: for positive sentiment\n",
    "1: for negative sentiment\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "After producing the translation of the english system prompt, I will swap the french equivalent and then conduct the experiment.\n",
    "\n",
    "this process was repeated for all of the native language challenges on a per model basis to ensure if anything difference in model translation is captured, and compared against the vanilla english system prompt for results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ddbede",
   "metadata": {},
   "source": [
    "**Generate Native French System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d5cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI based system prompt translation \n",
    "\n",
    "openai_french_sys_prompt = \"\"\"\n",
    "Vous êtes un assistant utile, capable de lire des tweets en français et en anglais, et de déterminer le sentiment de leurs auteurs.\n",
    "\n",
    "Après avoir lu un tweet et analysé attentivement son sentiment, vous répondez UNIQUEMENT avec :\n",
    "\n",
    "0 : pour un sentiment positif\n",
    "1 : pour un sentiment négatif\n",
    "\"\"\"\n",
    "\n",
    "# Mistral_AI based system prompt translation\n",
    "\n",
    "mistral_french_sys_prompt = \"\"\"\n",
    "Tu es un assistant utile, tu peux lire les tweets en français et en anglais et déterminer le sentiment de l'auteur.\n",
    "\n",
    "Après avoir lu un tweet et déterminé soigneusement son sentiment, tu réponds avec UNIQUEMENT :\n",
    "\n",
    "0 : pour un sentiment positif\n",
    "1 : pour un sentiment négatif\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a19a7",
   "metadata": {},
   "source": [
    "**Generate Native German System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a371384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI system prompt translation \n",
    "\n",
    "openai_german_sys_prompt = \"\"\"\n",
    "Du bist ein hilfreicher Assistent, der deutsche und englische Tweets lesen und die Stimmung der Verfasser bestimmen kann.\n",
    "\n",
    "Nachdem du einen Tweet gelesen und die Stimmung sorgfältig analysiert hast, antwortest du NUR mit:\n",
    "\n",
    "0: für positive Stimmung\n",
    "1: für negative Stimmung\n",
    "\"\"\"\n",
    "\n",
    "# Mistral_AI system prompt translation \n",
    "\n",
    "mistral_german_sys_prompt = \"\"\"\n",
    "Du bist ein hilfreicher Assistent, du kannst französische und englische Tweets lesen und die Stimmung des Schreibers bestimmen.\n",
    "\n",
    "Nach dem Lesen eines Tweets und der sorgfältigen Bestimmung seiner Stimmung antwortest du mit NUR:\n",
    "\n",
    "0: für positive Stimmung\n",
    "1: für negative Stimmung\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f159e22",
   "metadata": {},
   "source": [
    "**Generate Native Hindi System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cbc700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI translated system prompt\n",
    "\n",
    "openai_hindi_sys_prompt = \"\"\"\n",
    "आप एक सहायक सहायक हैं जो हिंदी और अंग्रेजी ट्वीट्स पढ़ सकते हैं और लेखकों की भावना का निर्धारण कर सकते हैं।  \n",
    "\n",
    "एक ट्वीट पढ़ने और भावना का सावधानीपूर्वक विश्लेषण करने के बाद, आप केवल इस प्रकार उत्तर देते हैं:  \n",
    "\n",
    "0: सकारात्मक भावना के लिए  \n",
    "1: नकारात्मक भावना के लिए  \n",
    "\"\"\"\n",
    "\n",
    "# Mistral_AI translated system prompt\n",
    "\n",
    "mistral_hindi_sys_prompt = \"\"\"\n",
    "आप एक मददगार सहायक हैं जो हिंदी और अंग्रेजी ट्वीट्स पढ़ सकते हैं और लेखक के भाव का निर्धारण कर सकते हैं।\n",
    "\n",
    "एक ट्वीट पढ़ने के बाद और भाव का सावधानीपूर्वक विश्लेषण करने के बाद, आप केवल इनमें से जवाब देते हैं:\n",
    "\n",
    "0: सकारात्मक भाव के लिए\n",
    "1: नकारात्मक भाव के लिए\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001008cb",
   "metadata": {},
   "source": [
    "**Generate Native Italian System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI translated system prompt\n",
    "\n",
    "openai_italian_sys_prompt = \"\"\"\n",
    "Sei un assistente utile, in grado di leggere tweet in italiano e inglese e determinare il sentimento degli autori.  \n",
    "\n",
    "Dopo aver letto un tweet e analizzato attentamente il suo sentimento, rispondi SOLO con:  \n",
    "\n",
    "0: per un sentimento positivo  \n",
    "1: per un sentimento negativo  \n",
    "\"\"\"\n",
    "\n",
    "# Mistral_AI translated system prompt\n",
    "\n",
    "mistral_italian_sys_prompt = \"\"\"\n",
    "Sei un assistente utile, puoi leggere tweet in italiano e in inglese e determinare il sentimento dell'autore.\n",
    "\n",
    "Dopo aver letto un tweet e averne attentamente determinato il sentimento, rispondi con SOLO:\n",
    "\n",
    "0: per sentimento positivo\n",
    "1: per sentimento negativo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b53e21",
   "metadata": {},
   "source": [
    "**Generate Native Portugeses System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI translated system prompt\n",
    "\n",
    "openai_portuguese_sys_prompt = \"\"\"\n",
    "Você é um assistente útil, capaz de ler tweets em português e inglês e determinar o sentimento dos autores.\n",
    "\n",
    "Após ler um tweet e analisar cuidadosamente seu sentimento, você responde SOMENTE com:\n",
    "\n",
    "0: para sentimento positivo\n",
    "1: para sentimento negativo\n",
    "\"\"\"\n",
    "\n",
    "# Mistral_AI translated system prompt\n",
    "\n",
    "mistral_portuguese_sys_prompt = \"\"\"\n",
    "Você é um assistente útil, capaz de ler tweets em português e inglês e determinar o sentimento dos autores.\n",
    "\n",
    "Depois de ler um tweet e analisar cuidadosamente seu sentimento, você responde APENAS com:\n",
    "\n",
    "0: para sentimento positivo\n",
    "1: para sentimento negativo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab811310",
   "metadata": {},
   "source": [
    "**Generate Native Spanish System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b1f53b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI translated system prompt:\n",
    "\n",
    "openai_spanish_sys_prompt = \"\"\"\n",
    "Eres un asistente útil, puedes leer tweets en español e inglés y determinar el sentimiento de los autores.\n",
    "\n",
    "Después de leer un tweet y analizar cuidadosamente su sentimiento, respondes SOLO con:\n",
    "\n",
    "0: para sentimiento positivo\n",
    "1: para sentimiento negativo\n",
    "\"\"\"\n",
    "\n",
    "# Mistral_AI translated system prompt:\n",
    "\n",
    "mistral_spanish_sys_prompt = \"\"\"\n",
    "Eres un asistente útil, puedes leer tweets en español e inglés y determinar el sentimiento del escritor.\n",
    "\n",
    "Después de leer un tweet y determinar cuidadosamente su sentimiento, respondes con SOLO:\n",
    "\n",
    "0: para sentimiento positivo\n",
    "1: para sentimiento negativo\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97feb67f",
   "metadata": {},
   "source": [
    "**Create Native Language ChatPromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a8f5b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "native_language_prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"{system_prompt}\"),\n",
    "    (\"human\", \"{tweet}: \")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cbb9df",
   "metadata": {},
   "source": [
    "**Create Native Language ChatPrompTemplate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7232cd1",
   "metadata": {},
   "source": [
    "**OpenAI GPT 4o mini test using Native ChatPromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d56f43ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# build the open_ai_4o_mini chain\n",
    "open_ai_tweet_native_sentiment_chain = native_language_prompt_template | open_ai_gpt_4o_mini | StrOutputParser()\n",
    "\n",
    "# invoke the chain with a test tweet \n",
    "\n",
    "# spanish_tweet:  \"Estoy triste porque no puedo hablar español con fluidez.\"\n",
    "# translation: \"I am sad I can't speak spanish fluently\"\n",
    "open_ai_tweet_native_sentiment_chain.invoke({\"system_prompt\": openai_spanish_sys_prompt, \"tweet\": \"Estoy triste porque no puedo hablar español con fluidez.\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b195afda",
   "metadata": {},
   "source": [
    "**Mistral Nemo Native ChatPromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4e1122b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# build the open_ai_4o_mini chain\n",
    "mistral_nemo_tweet_native_sentiment_chain = native_language_prompt_template | mistral_nemo | StrOutputParser()\n",
    "\n",
    "# invoke the chain with a test tweet \n",
    "\n",
    "# spanish_tweet:  \"Estoy triste porque no puedo hablar español con fluidez.\"\n",
    "# translation: \"I am sad I can't speak spanish fluently\"\n",
    "mistral_nemo_tweet_native_sentiment_chain.invoke({\"system_prompt\": mistral_spanish_sys_prompt, \"tweet\": \"Estoy triste porque no puedo hablar español con fluidez.\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
